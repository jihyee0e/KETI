# ğŸ§ª ReLU vs SiLU ì„±ëŠ¥ ë¹„êµ
# âœ… ì‹¤í—˜ ê°œìš”
# ë°ì´í„°ì…‹: MNIST ì†ê¸€ì”¨ ìˆ«ì ì´ë¯¸ì§€
# ëª¨ë¸: ì‘ì€ MLP (Multi-Layer Perceptron)
# ì°¨ì´ì : ReLU vs SiLUë§Œ ë°”ê¿”ì„œ ì •í™•ë„ ë¹„êµ
# ëª©ì : ë‹¨ìˆœ ëª¨ë¸ì—ì„œë„ ì‹¤ì œ ì„±ëŠ¥ ì°¨ì´ê°€ ëˆˆì— ë„ê²Œ í°ê°€?

import torch
import torch.nn as nn
import torch.optim as optim
from torchvision import datasets, transforms
from torch.utils.data import DataLoader
# ë°ì´í„°ì…‹ ë¡œë”©

transform = transforms.ToTensor()
train_dataset = datasets.MNIST(root='./data', train=True, transform=transform, download=True)
test_dataset  = datasets.MNIST(root='./data', train=False, transform=transform, download=True)

train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)
test_loader  = DataLoader(test_dataset, batch_size=1000)

# ëª¨ë¸ ì •ì˜
class SimpleMLP(nn.Module):
    def __init__(self, activation_fn):
        super().__init__()
        self.model = nn.Sequential(
            nn.Flatten(),  # 28x28 ì´ë¯¸ì§€ë¥¼ ë²¡í„°ë¡œ ë³€í™˜
            nn.Linear(28*28, 256),  # ì€ë‹‰ì¸µ1
            activation_fn,  # ì™¸ë¶€ì—ì„œ ReLU() or SiLU() ë„£ì–´ì¤„ ê²ƒì„
            nn.Linear(256, 128),  # ì€ë‹‰ì¸µ2
            activation_fn,
            nn.Linear(128, 10)  # MNIST í´ë˜ìŠ¤ 10ê°œ(0~9 ìˆ«ì) ì¶œë ¥
        )

    def forward(self, x):
        return self.model(x)

# í•™ìŠµ ë° í…ŒìŠ¤íŠ¸
def train_and_evaluate(activation_fn, name):
    # ReLU ëª¨ë¸ì´ ë ì§€, SiLU ëª¨ë¸ì´ ë ì§€ ê²°ì •
    model = SimpleMLP(activation_fn).to(device)  # í•™ìŠµì„ GPUì—ì„œ ëŒë¦´ì§€, CPUì—ì„œ ëŒë¦´ì§€ ìë™ ì„¤ì •
    criterion = nn.CrossEntropyLoss()  # ëª¨ë¸ì´ ì–¼ë§ˆë‚˜ í‹€ë ¸ëŠ”ì§€ ê³„ì‚°í•˜ëŠ” í•¨ìˆ˜
    optimizer = optim.Adam(model.parameters(), lr=0.001)  # Adam: ë¹ ë¥´ê²Œ ìˆ˜ë ´

    for epoch in range(5):  # 5ë²ˆë§Œ í•™ìŠµ
        model.train()  # í•™ìŠµ ì¤‘ 
        for x, y in train_loader:
            x, y = x.to(device), y.to(device)  # x(ì´ë¯¸ì§€ ë°ì´í„°), y(ì •ë‹µ ìˆ«ì)
            optimizer.zero_grad()  # ê¸°ìš¸ê¸° ì´ˆê¸°í™” (ì´ì „ gradientë‘ ì„ì´ë©´ ì•ˆë¨)
            loss = criterion(model(x), y)  # ëª¨ë¸í•œí…Œ x ë„£ì–´ë³´ê³  ì˜ˆì¸¡ ê²°ê³¼ ë°˜í™˜ë°›ìŒ -> ì •ë‹µì´ë‘ ì–¼ë§ˆë‚˜ ì°¨ì´ë‚˜ëŠ”ì§€ ê³„ì‚°
            loss.backward()  # ì†ì‹¤ì´ ì–¼ë§ˆë‚˜ ë°œìƒí–ˆëŠ”ì§€ë¥¼ ê° ê°€ì¤‘ì¹˜ë³„ë¡œ ì—­ì „íŒŒë¡œ ê³„ì‚°(ì–´ëŠ ê°€ì¤‘ì¹˜ë¥¼ ì–¼ë§ˆë‚˜ ë°”ê¾¸ë©´ ì†ì‹¤ ì¤„ì¼ ìˆ˜ ìˆì„ì§€)
            optimizer.step()  # ìœ„ì—ì„œ ê³„ì‚°í•œ gradientë¥¼ ì‚¬ìš©í•´ ì‹¤ì œë¡œ ê°€ì¤‘ì¹˜ë¥¼ ì—…ë°ì´íŠ¸

    # í…ŒìŠ¤íŠ¸ ì„±ëŠ¥ ì¸¡ì •
    model.eval()  # í•™ìŠµ ì¤‘ì´ ì•„ë‹˜ì„ ëª…ì‹œ
    correct = total = 0
    with torch.no_grad():  # í…ŒìŠ¤íŠ¸ í•  ë•ŒëŠ” gradient ê³„ì‚°x -> ë©”ëª¨ë¦¬ ì•„ë¼ê³  ì†ë„ í–¥ìƒ
        for x, y in test_loader:  # ë°°ì¹˜ ë‹¨ìœ„ë¡œ ë¶ˆëŸ¬ì™€ì„œ ë°˜ë³µ
            x, y = x.to(device), y.to(device)
            pred = model(x).argmax(dim=1)  # ëª¨ë¸ ì˜ˆì¸¡ ê²°ê³¼ â†’ ìˆ«ì 0~9ì— ëŒ€í•œ ì ìˆ˜ë“¤ ì¤‘ ì ìˆ˜ê°€ ê°€ì¥ ë†’ì€ ê²ƒ(ëª¨ë¸ì´ ì˜ˆì¸¡í•œ ìˆ«ì)
            correct += (pred == y).sum().item()  # ì˜ˆì¸¡ì´ ë§ì•˜ëŠ”ì§€ í™•ì¸í•˜ê³  ë§ì€ ê°œìˆ˜ë§Œ ì…ˆ
            total += y.size(0)

    acc = correct / total
    print(f"{name} Accuracy: {acc:.4f}")

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

train_and_evaluate(nn.ReLU(), "ReLU")
train_and_evaluate(nn.SiLU(), "SiLU")

