# chatbot_view.py
import streamlit as st
from components.file_manager import handle_file_upload
from agent.chatbot_agent import create_agent  # agent ìƒì„± í•¨ìˆ˜
import ollama
from langchain_ollama import ChatOllama
from utils.common import stored_dfs

def run_agent(agent, user_input: str) -> str:
    try:
        result = agent.invoke({"input": user_input}, return_intermediate_steps=True)
        steps = result.get("intermediate_steps", [])
        final_output = result.get("output", "").strip()
        response_parts = []

        # intermediate_steps ê²°ê³¼ ë¨¼ì € ìˆ˜ì§‘
        for step in steps:
            tool_result = step[1]
            if isinstance(tool_result, str):
                cleaned = tool_result.strip()
                if cleaned and cleaned != final_output:
                    response_parts.append(cleaned)

        # ìµœì¢… ì‘ë‹µ ì¶”ê°€ (ì¤‘ë³µ ë°©ì§€ë¨)
        if final_output:
            response_parts.append(final_output)

        if not response_parts:
            return "âš ï¸ ë„êµ¬ê°€ ì‹¤í–‰ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ëª¨ë¸ì´ íˆ´ì„ í˜¸ì¶œí•˜ì§€ ì•Šì•˜ê±°ë‚˜, ì§€ì‹œë¥¼ ë”°ë¥´ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."

        return "\n\n".join(response_parts)

    except Exception as e:
        return f"âŒ ì˜¤ë¥˜ ë°œìƒ: {e}"

# ì‚¬ì´ë“œë°” íŒŒì¼ ì—…ë¡œë“œ
with st.sidebar:
    st.header("ğŸ¤– ëª¨ë¸ ì„ íƒ")
    models_info = ollama.list()
    models = models_info.get("models", [])
    if not models:
        st.error("ì‚¬ìš© ê°€ëŠ¥í•œ LLM ëª¨ë¸ì´ ì—†ìŠµë‹ˆë‹¤. Ollama ì„œë²„ë¥¼ í™•ì¸í•˜ì„¸ìš”.")
        st.stop()
    model_names = [m.model for m in models] 
    model_names.sort()

    # ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™”
    if "selected_model" not in st.session_state:
        st.session_state["selected_model"] = model_names[1]  # í•œêµ­ì–´ ìµœì í™” ëª¨ë¸

    selected_model = st.selectbox(
        "LLM ëª¨ë¸ì„ ì„ íƒí•´ì£¼ì„¸ìš”.",
        model_names,
        index=model_names.index(st.session_state["selected_model"]),
    )

    st.header("ğŸ“‚ ë°ì´í„° ì—…ë¡œë“œ")
    uploaded_files = st.file_uploader(
        "ì—…ë¡œë“œ (csv/xls[x]/json/parquet/pdf/txt)",
        type=["csv", "xls", "xlsx", "json", "parquet", "pdf", "txt"],
        accept_multiple_files=True,
    )
    df, text_only, file = handle_file_upload(uploaded_files)

    # ë””ë²„ê¹…
    st.sidebar.text(f"[DEBUG] df is {'loaded' if df is not None else 'missing'}")

    if df is not None:
        stored_dfs["df"] = df

        if "agent_executor" not in st.session_state:
            llm = ChatOllama(model=selected_model, temperature=0)
            st.session_state["agent_executor"] = create_agent(llm, df=df)

    if selected_model != st.session_state["selected_model"]:
        st.session_state["selected_model"] = selected_model
        llm = ChatOllama(model=st.session_state["selected_model"], temperature=0)
        st.session_state["agent_executor"] = create_agent(ChatOllama(model=selected_model, temperature=0), df=df)
        st.session_state["messages"] = []

# ì±—ë´‡ UI
st.subheader("ğŸ’¬ LLM ì±—ë´‡")
if "messages" not in st.session_state:
    st.session_state["messages"] = []

for msg in st.session_state["messages"]:
    with st.chat_message(msg["role"]):
        st.markdown(msg["content"])

user_input = st.chat_input("ë¬´ì—‡ì„ ë„ì™€ë“œë¦´ê¹Œìš”?")
if user_input:
    st.session_state["messages"].append({"role": "user", "content": user_input})
    with st.chat_message("user"):
        st.markdown(user_input)

    with st.chat_message("assistant"):
        with st.spinner("Thinking..."):
            try:
                agent = st.session_state["agent_executor"]
                output = run_agent(agent, user_input)
            except Exception as e:
                output = f"âŒ ì˜¤ë¥˜ ë°œìƒ: {e}"

            st.markdown(output)
            st.session_state["messages"].append({"role": "assistant", "content": output})


# import streamlit as st
# from langchain.agents import AgentExecutor
# from langchain_core.messages import AIMessage, HumanMessage
# from langchain_experimental.tools.python.tool import PythonREPLTool
# from langchain.agents import create_json_chat_agent
# from langchain import hub
# import os
# from dotenv import load_dotenv
# from agent.tool import get_all_tools  # ì‚¬ìš©ìê°€ ì •ì˜í•œ ì»¤ìŠ¤í…€ tool ëª¨ìŒ
# from utils.common import stored_dfs    # df ì €ì¥ì†Œ (session ìœ ì§€ìš©)
# from langchain_community.llms import Ollama  # or ChatOllama
# from components.file_manager import handle_file_upload
# import streamlit as st

# st.set_page_config(page_title="ì±—ë´‡", layout="wide")

# load_dotenv()

# os.environ["LANGCHAIN_TRACING_V2"] = "true"
# os.environ["LANGCHAIN_ENDPOINT"] = "https://api.smith.langchain.com"
# langchain_key = os.getenv("LANGCHAIN_API_KEY")   # .env ë“±ë¡

# with st.sidebar:
#     st.header("ğŸ“‚ ë°ì´í„° ì—…ë¡œë“œ")
#     uploaded_files = st.file_uploader(
#         "ì—…ë¡œë“œ (csv/xls[x]/json/parquet/pdf/txt)",
#         type=["csv", "xls", "xlsx", "json", "parquet", "pdf", "txt"],
#         accept_multiple_files=True,
#     )
#     # íŒŒì¼ ì²˜ë¦¬ í•¨ìˆ˜ í˜¸ì¶œ (app.pyì˜ handle_file_upload ì¬ì‚¬ìš©)
#     df, text_only, file = handle_file_upload(uploaded_files)

# st.subheader("ğŸ’¬ LLM ì±—ë´‡")

# if "message" not in st.session_state:
#     st.session_state("messages") = []

# # ê¸°ì¡´ ë©”ì‹œì§€ ì¶œë ¥
# for msg in st.session_state["messages"]:
#     with st.chat_message(msg["role"]):
#         st.markdown(msg["content"])

# user_input = st.chat_input("ë¬´ì—‡ì„ ë„ì™€ë“œë¦´ê¹Œìš”?")
# if user_input:
#     st.session_state["messages"].append({"role": "user", "content": user_input})
#     with st.chat_message("user"):
#         st.markdown(user_input)

# if user_input:
#     with st.chat_message("assistant"):
#         with st.spinner("Thinking..."):
#             try:
#                 # ì—…ë¡œë“œëœ ë°ì´í„°ê°€ ìˆìœ¼ë©´ agentì— ë„˜ê¹€
#                 agent = create_agent(llm, df=df)
#                 result = agent.invoke({"input": user_input})
#                 output = result.get("output", "âš ï¸ ì‘ë‹µ ì—†ìŒ")
#             except Exception as e:
#                 output = f"âŒ ì˜¤ë¥˜ ë°œìƒ: {e}"

#             st.markdown(output)
#             st.session_state["messages"].append({"role": "assistant", "content": output})


# def create_agent(llm, df=None):
#     tools = get_all_tools()

#     if df is not None:
#         stored_dfs["df"] = df
#         tools.append(PythonREPLTool(locals={"df": df}))

#     prompt = hub.pull("test-template")
#     agent = create_json_chat_agent(llm, tools, prompt)

#     return AgentExecutor(
#         agent=agent,
#         tools=tools,
#         verbose=True,
#         max_iterations=5,
#         max_execution_time=300,
#         handle_parsing_errors=True,
#         return_intermediate_steps=False,
#     )

# def render(df, text_only, llm):
#     st.subheader("ğŸ’¬ LLM ì±—ë´‡")

#     if "messages" not in st.session_state:
#         st.session_state["messages"] = []

#     for msg in st.session_state["messages"]:
#         with st.chat_message(msg["role"]):
#             st.markdown(msg["content"])

#     user_input = st.chat_input("ë¬´ì—‡ì„ ë„ì™€ë“œë¦´ê¹Œìš”?")
#     if user_input:
#         st.session_state["messages"].append({"role": "user", "content": user_input})
#         with st.chat_message("user"):
#             st.markdown(user_input)

#         with st.chat_message("assistant"):
#             with st.spinner("Thinking..."):
#                 try:
#                     agent = create_agent(llm, df=df)
#                     result = agent.invoke({"input": user_input})
#                     output = result.get("output", "âš ï¸ ì‘ë‹µ ì—†ìŒ")
#                 except Exception as e:
#                     output = f"âŒ ì˜¤ë¥˜ ë°œìƒ: {e}"

#                 st.markdown(output)
#                 st.session_state["messages"].append({"role": "assistant", "content": output})
